{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leveraging Linguistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to pick up a simple use case and see how we can solve that. Then, we repeat this again, but on a slighlty different text corpus and so on. \n",
    "\n",
    "This helps us learn build intuition on how to use linguistics in NLP. As mentioned, I am going to use spaCy here, but you are free to use NLTK or anything else available in your favourite progamming language\n",
    "\n",
    "## Grammar Crash Course and spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirant/miniconda3/envs/nlpbook/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/nirant/miniconda3/envs/nlpbook/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n",
      "/home/nirant/miniconda3/envs/nlpbook/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/nirant/miniconda3/envs/nlpbook/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192, got 176\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy # for visualization\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is an error above, try:\n",
    "- Windows Shell:```python -m spacy download en``` as **Administrator**\n",
    "- Linux Terminal:```sudo python -m spacy download en ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first half talks about NLP pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redacting Names with Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Madam Pomfrey, the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. Ginny Weasley, who had been looking pale, was bullied into taking some by Percy.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the text with spaCy. This runs the entire pipeline.\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pomfrey (PERSON)\n",
      "Pepperup (ORG)\n",
      "several hours (TIME)\n",
      "Ginny Weasley (PERSON)\n",
      "Percy (PERSON)\n"
     ]
    }
   ],
   "source": [
    "# 'doc' now contains a parsed version of text. We can use it to do anything we want!\n",
    "# For example, this will print out all the named entities that were detected:\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: Explain the entity and entity labels above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_names(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_sentence = []\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            redacted_sentence.append(\"[REDACTED]\")\n",
    "        else:\n",
    "            redacted_sentence.append(token.string)\n",
    "    return \"\".join(redacted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam [REDACTED], the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. [REDACTED][REDACTED], who had been looking pale, was bullied into taking some by [REDACTED].'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redact_names(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_names(text):\n",
    "    doc = nlp(text)\n",
    "    redacted_sentence = []\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "    for token in doc:\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            redacted_sentence.append(\"[REDACTED]\")\n",
    "        else:\n",
    "            redacted_sentence.append(token.string)\n",
    "    return \"\".join(redacted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Madam [REDACTED], the nurse, was kept busy by a sudden spate of colds among the staff and students. Her Pepperup potion worked instantly, though it left the drinker smoking at the ears for several hours afterward. [REDACTED], who had been looking pale, was bullied into taking some by [REDACTED].'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redact_names(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Types "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_text_entities(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        print(f'{ent}, Label: {ent.label_}, {spacy.explain(ent.label_)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla, Label: ORG, Companies, agencies, institutions, etc.\n",
      "20%, Label: PERCENT, Percentage, including \"%\"\n",
      "the months, Label: DATE, Absolute or relative dates or periods\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Tesla has gained 20% market share in the months since')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taj Mahal, Label: PERSON, People, including fictional\n",
      "Mughal, Label: NORP, Nationalities or religious or political groups\n",
      "Shah Jahan, Label: PERSON, People, including fictional\n",
      "Yamuna, Label: LOC, Non-GPE locations, mountain ranges, bodies of water\n",
      "Agra, Label: GPE, Countries, cities, states\n",
      "India, Label: GPE, Countries, cities, states\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Taj Mahal built by Mughal Emperor Shah Jahan stands tall on the banks of Yamuna in modern day Agra, India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashoka, Label: PERSON, People, including fictional\n",
      "Indian, Label: NORP, Nationalities or religious or political groups\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('Ashoka was a great Indian king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ashoka University, Label: ORG, Companies, agencies, institutions, etc.\n",
      "the Young India Fellowship, Label: ORG, Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "explain_text_entities('The Ashoka University sponsors the Young India Fellowship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation with PoS Tagging\n",
    "\n",
    "Sometimes, we want to quickly pull out keywords, or keyphrases from a larger body of text. This helps us mentally paint a picture of what this text is about. This is particularly helpful in analysis of texts like long emails or essays. \n",
    "\n",
    "We refer to these as noun chunks. Noun chunks are _noun phrases_ - not a single word, but a short phrase which describes the noun. For example, \"the blue skies\" or \"the worldâ€™s largest conglomerate\". \n",
    "\n",
    "To get the noun chunks in a document, simply iterate over `doc.noun_chunks`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = 'Bansoori is an Indian classical instrument. Tom plays Bansoori and Guitar.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Bansoori\n",
      "1 an Indian classical instrument\n",
      "2 Tom\n",
      "2 Bansoori\n",
      "2 Guitar\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(doc.sents):\n",
    "    for noun in sentence.noun_chunks:\n",
    "        print(idx+1, noun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example text has two sentences, we can pull out noun phrase chunks from each sentence. We pull out noun phrases instead of single words. This means, we are able to pull out 'an Indian classical instrument' as one noun. This is quite useful as we will see in a moment.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's take a quick look at all parts-of-speech tags in our example text. We will use the verbs and adjectives to write some simple question generating logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori PROPN NNP\n",
      "is VERB VBZ\n",
      "an DET DT\n",
      "Indian ADJ JJ\n",
      "classical ADJ JJ\n",
      "instrument NOUN NN\n",
      ". PUNCT .\n",
      "Tom PROPN NNP\n",
      "plays VERB VBZ\n",
      "Bansoori PROPN NNP\n",
      "and CCONJ CC\n",
      "Guitar PROPN NNP\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.pos_, token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that here 'instrument' is tagged as a NOUN while 'Indian' and 'classical' are tagged as adjectives. This makes sense. Addititionally, Bansoor and Guitar are tagged as PROPN or Proper Nouns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nouns vs Proper Noun** \n",
    "Nouns name people, places, and things. Common nouns name general items like waiter, jeans, country. Proper nouns name specific things like Roger, Levi's, India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruleset = [\n",
    "    {\n",
    "        'id': 1, \n",
    "        'req_tags': ['NNP', 'VBZ', 'NN'],\n",
    "    }, \n",
    "    {\n",
    "        'id': 2, \n",
    "        'req_tags': ['NNP', 'VBZ'],\n",
    "    }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 1, 'req_tags': ['NNP', 'VBZ', 'NN']}, {'id': 2, 'req_tags': ['NNP', 'VBZ']}]\n"
     ]
    }
   ],
   "source": [
    "print(ruleset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(doc, tag):\n",
    "    return [tok for tok in doc if tok.tag_ == tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori is an Indian classical instrument.\n",
      "What is Bansoori?\n",
      "Tom plays Bansoori and Guitar.\n",
      "What does Tom play?\n"
     ]
    }
   ],
   "source": [
    "def sent_to_ques(sent):\n",
    "    doc = nlp(sent)\n",
    "    pos_tags = [token.tag_ for token in doc]\n",
    "    for idx, rule in enumerate(ruleset):\n",
    "        if rule['id'] == 1:\n",
    "            if all(key in pos_tags for key in rule['req_tags']): \n",
    "                print(sent)\n",
    "                NNP = get_pos_tag(doc, \"NNP\")\n",
    "                NNP = str(NNP[0])\n",
    "                VBZ = get_pos_tag(doc, \"VBZ\")\n",
    "#                 subjects = get_subjects_of_verb(VBZ[0])\n",
    "#                 print(subjects)\n",
    "                VBZ = str(VBZ[0])\n",
    "                ques = f'What {VBZ} {NNP}?'\n",
    "                return(ques)\n",
    "        if rule['id'] == 2:\n",
    "            if all(key in pos_tags for key in rule['req_tags']): #'NNP', 'VBZ' in sentence.\n",
    "                print(sent)\n",
    "                NNP = get_pos_tag(doc, \"NNP\")\n",
    "                NNP = str(NNP[0])\n",
    "                VBZ = get_pos_tag(doc, \"VBZ\")\n",
    "                VBZ = str(VBZ[0].lemma_)\n",
    "                ques = f'What does {NNP} {VBZ}?'\n",
    "                return (ques)\n",
    "\n",
    "for sent in doc.sents:\n",
    "    print(sent_to_ques(str(sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Generation using Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_verbs_of_sent(sent):\n",
    "    \"\"\"Return the main (non-auxiliary) verbs in a sentence.\"\"\"\n",
    "    return [tok for tok in sent\n",
    "            if tok.pos_ == 'VERB' and tok.dep_ not in {'aux', 'auxpass'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_conjuncts(tok):\n",
    "    \"\"\"\n",
    "    Return conjunct dependents of the leftmost conjunct in a coordinated phrase,\n",
    "    e.g. \"Burton, [Dan], and [Josh] ...\".\n",
    "    \"\"\"\n",
    "    return [right for right in tok.rights\n",
    "            if right.dep_ == 'conj']\n",
    "\n",
    "def get_subjects_of_verb(verb):\n",
    "    SUBJ_DEPS = {'agent', 'csubj', 'csubjpass', 'expl', 'nsubj', 'nsubjpass'}\n",
    "    \"\"\"Return all subjects of a verb according to the dependency parse.\"\"\"\n",
    "    subjs = [tok for tok in verb.lefts\n",
    "             if tok.dep_ in SUBJ_DEPS]\n",
    "    # get additional conjunct subjects\n",
    "    subjs.extend(tok for subj in subjs for tok in _get_conjuncts(subj))\n",
    "    return subjs\n",
    "\n",
    "def get_objects_of_verb(verb):\n",
    "    \"\"\"\n",
    "    Return all objects of a verb according to the dependency parse,\n",
    "    including open clausal complements.\n",
    "    \"\"\"\n",
    "    OBJ_DEPS = {'attr', 'dobj', 'dative', 'oprd'}\n",
    "    objs = [tok for tok in verb.rights\n",
    "            if tok.dep_ in OBJ_DEPS]\n",
    "    # get open clausal complements (xcomp)\n",
    "    objs.extend(tok for tok in verb.rights\n",
    "                if tok.dep_ == 'xcomp')\n",
    "    # get additional conjunct objects\n",
    "    objs.extend(tok for obj in objs for tok in _get_conjuncts(obj))\n",
    "    return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori nsubj\n",
      "is ROOT\n",
      "an det\n",
      "Indian amod\n",
      "classical amod\n",
      "instrument attr\n",
      ". punct\n",
      "Tom nsubj\n",
      "plays ROOT\n",
      "Bansoori dobj\n",
      "and cc\n",
      "Guitar conj\n",
      ". punct\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bansoori is an Indian classical instrument. is be [Bansoori] [instrument]\n",
      "Tom plays Bansoori and Guitar. plays play [Tom] [Bansoori, Guitar]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(example_text)\n",
    "for sentence in doc.sents:\n",
    "    print(sentence, sentence.root, sentence.root.lemma_, get_subjects_of_verb(sentence.root), get_objects_of_verb(sentence.root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is Bansoori?', 'answers': [instrument]},\n",
       " {'question': 'What does Tom play?', 'answers': [Bansoori, Guitar]}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def para_to_ques(eg_text):\n",
    "    doc = nlp(eg_text)\n",
    "    results = []\n",
    "    for sentence in doc.sents:\n",
    "        root = sentence.root\n",
    "        ask_about = get_subjects_of_verb(root)\n",
    "        answers = get_objects_of_verb(root)\n",
    "        if len(ask_about) > 0 and len(answers) > 0:\n",
    "            if root.lemma_ == \"be\":\n",
    "                question = f'What {root} {ask_about[0]}?'\n",
    "            else:\n",
    "                question = f'What does {ask_about[0]} {root.lemma_}?'\n",
    "            results.append({'question':question, 'answers':answers})\n",
    "    return results\n",
    "para_to_ques(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_example_text = \"\"\"\n",
    "Puliyogare is a South Indian dish made of rice and tamarind. \n",
    "Priya writes poems. Shivangi bakes cakes. Sachin sings in the orchestra.\n",
    "\n",
    "Osmosis is the movement of a solvent across a semipermeable membrane toward a higher concentration of solute. In biological systems, the solvent is typically water, but osmosis can occur in other liquids, supercritical liquids, and even gases.\n",
    "When a cell is submerged in water, the water molecules pass through the cell membrane from an area of low solute concentration to high solute concentration. For example, if the cell is submerged in saltwater, water molecules move out of the cell. If a cell is submerged in freshwater, water molecules move into the cell.\n",
    "\n",
    "Raja-Yoga is divided into eight steps. The first is Yama. Yama is nonviolence, truthfulness, continence, and non-receiving of any gifts.\n",
    "After Yama, Raja-Yoga has Niyama. cleanliness, contentment, austerity, study, and self - surrender to God.\n",
    "The steps are Yama and Niyama. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is Puliyogare?', 'answers': [dish]},\n",
       " {'question': 'What does Priya write?', 'answers': [poems]},\n",
       " {'question': 'What does Shivangi bake?', 'answers': [cakes]},\n",
       " {'question': 'What is Osmosis?', 'answers': [movement]},\n",
       " {'question': 'What is solvent?', 'answers': [water]},\n",
       " {'question': 'What is first?', 'answers': [Yama]},\n",
       " {'question': 'What is Yama?',\n",
       "  'answers': [nonviolence, truthfulness, continence, of]},\n",
       " {'question': 'What does Yoga have?', 'answers': [Niyama]},\n",
       " {'question': 'What are steps?', 'answers': [Yama, Niyama]}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_to_ques(large_example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facts Extraction using Semi Structured Sentence Parsing\n",
    "Introducing textacy,\n",
    "\n",
    "Boss mode with co reference resolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nlpbook]",
   "language": "python",
   "name": "conda-env-nlpbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
