{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "This is a code walkthrough for self-starters on most text cleaning task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have always liked **The Adventures of Sherlock Holmes** by _Arthur Conan Doyle_. Let's download the book and save it locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.gutenberg.org/ebooks/1661.txt.utf-8'\n",
    "file_name = 'sherlock.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "# Download the file from `url` and save it locally under `file_name`:\n",
    "\n",
    "with urllib.request.urlopen(url) as response:\n",
    "    with open(file_name, 'wb') as out_file:\n",
    "        data = response.read() # a `bytes` object\n",
    "        out_file.write(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sherlock.txt\n"
     ]
    }
   ],
   "source": [
    "!ls {*.txt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿Project Gutenberg's The Adventures of Sherlock Holmes, by Arthur Conan Doyle\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, give it away or\n",
      "re-use it under the terms of the Project Gutenberg License included\n"
     ]
    }
   ],
   "source": [
    "!head -5 sherlock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file contains header and footer information from Project Gutenberg. We are not interested in the same and will discard the copyright and other legal notices. \n",
    "\n",
    "Todo: \n",
    "- Open the file and delete the header and footer information and save the file as ```sherlock_clean.txt```\n",
    "\n",
    "I opened the text file to see that I need to remove the first 33 lines. Let's do that using shell commands - which also work on Windows inside Jupyter notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i 1,33d sherlock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the ```sed``` syntax. \n",
    "\n",
    "The ```-i``` flag tells to make the changes in place.  \n",
    "```1,33d``` instructs to delete lines 1 to 33."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ADVENTURES OF SHERLOCK HOLMES\n",
      "\n",
      "by\n",
      "\n",
      "SIR ARTHUR CONAN DOYLE\n"
     ]
    }
   ],
   "source": [
    "!head -5 sherlock.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration Notes\n",
    "\n",
    "\n",
    "Before I continue to text cleaning for any Natural Language Processing Task, I like to spend a few seconds taking a quick glance at the data itself. I noted down some of the things I spotted below, of course a trained eye can see a lot more than I did: \n",
    "\n",
    "1. Dates are written in a mixed format: `twentieth of March, 1888`, times are too: `three o'clock`\n",
    "1. Text is wrapped at around 70 columns, or no line can be longer than 70 characters \n",
    "1. There are lot of proper nouns. These include names such as `Atkinson`, `Trepoff` in addition to locations such as `Trinconmalee` and `Baker Street` etc.\n",
    "1. The index is in Roman numerals such as `I` and `IV` and not `1` or `4`\n",
    "1. There are lot of dialogues such as: \"You have carte blanche.\" with no narrative around them. This storytelling style switches freely from a narrative to a dialogue driven. \n",
    "1. The grammar and vocabulary is slightly unusual because of the time when Doyle wrote.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE A\n"
     ]
    }
   ],
   "source": [
    "#let's the load data to RAM\n",
    "text = open(file_name, 'r', encoding='utf-8').read()  # note that I add an encoding='utf-8' parameter to preserve information\n",
    "print(text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file is loaded as datatype: <class 'str'> and has 581204 characters in it\n"
     ]
    }
   ],
   "source": [
    "print(f'The file is loaded as datatype: {type(text)} and has {len(text)} characters in it')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Loaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '$', '%', '&', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'â', 'è', 'é']\n",
      "There are 85 unique characters, including both ASCII and Unicode character\n"
     ]
    }
   ],
   "source": [
    "# how many unique characters do we see? \n",
    "# For reference, ASCII has 127 characters in it - so we expect this to have at most 127 characters\n",
    "unique_chars = list(set(text))\n",
    "unique_chars.sort()\n",
    "print(unique_chars)\n",
    "print(f'There are {len(unique_chars)} unique characters, including both ASCII and Unicode character')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our machine learning models, we often need the words to occur as individual tokens or single words. This process is called:\n",
    "\n",
    "## Tokenization \n",
    "\n",
    "We convert the raw text into a list of words. This preserves the original ordering of the text. \n",
    "\n",
    "### Split by Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107431\n"
     ]
    }
   ],
   "source": [
    "words = text.split()\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman.', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name.', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex.', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler.', 'All', 'emotions,', 'and', 'that', 'one', 'particularly,', 'were', 'abhorrent', 'to', 'his', 'cold,', 'precise', 'but', 'admirably', 'balanced', 'mind.', 'He', 'was,', 'I', 'take', 'it,', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen,', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position.', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions,', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer.', 'They', 'were', 'admirable', 'things', 'for']\n"
     ]
    }
   ],
   "source": [
    "print(words[90:200])  #start with the first chapeter, ignoring the index for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red-headed', 'woman', 'on', 'the', 'street']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's look at another example: \n",
    "'red-headed woman on the street'.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the words red-headed were not split. This is something we may or may not want to keep always.  \n",
    "\n",
    "*Problem:* Punctuations are often appearing with the word itself, like: `Adler.` and `emotions,`.\n",
    "\n",
    "*Solution:* Simply extract words and discard everything else. This means we will discard all non-ASCII characters and punctuations.\n",
    "\n",
    "### Split by Word Extraction\n",
    "**Introducing Regex**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Words', 'words', 'words', '']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.split('\\W+', 'Words, words, words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions can be daunting at first, but are very powerful. The regular expression `\\W+` means *a word character (A-Z etc.) repeated one or more times*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_alphanumeric = re.split('\\W+', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(109111, 107431)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_alphanumeric), len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BOHEMIA', 'I', 'To', 'Sherlock', 'Holmes', 'she', 'is', 'always', 'THE', 'woman', 'I', 'have', 'seldom', 'heard', 'him', 'mention', 'her', 'under', 'any', 'other', 'name', 'In', 'his', 'eyes', 'she', 'eclipses', 'and', 'predominates', 'the', 'whole', 'of', 'her', 'sex', 'It', 'was', 'not', 'that', 'he', 'felt', 'any', 'emotion', 'akin', 'to', 'love', 'for', 'Irene', 'Adler', 'All', 'emotions', 'and', 'that', 'one', 'particularly', 'were', 'abhorrent', 'to', 'his', 'cold', 'precise', 'but', 'admirably', 'balanced', 'mind', 'He', 'was', 'I', 'take', 'it', 'the', 'most', 'perfect', 'reasoning', 'and', 'observing', 'machine', 'that', 'the', 'world', 'has', 'seen', 'but', 'as', 'a', 'lover', 'he', 'would', 'have', 'placed', 'himself', 'in', 'a', 'false', 'position', 'He', 'never', 'spoke', 'of', 'the', 'softer', 'passions', 'save', 'with', 'a', 'gibe', 'and', 'a', 'sneer', 'They', 'were', 'admirable']\n"
     ]
    }
   ],
   "source": [
    "print(words_alphanumeric[90:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice how `Adler` no longer has the punctuation with her. This is what we wanted. Mission Accomplished.  \n",
    "\n",
    "**What was the tradeoff we made here?** To understand that, let's look at another example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Isn', 't', 'he', 'coming', 'home', 'for', 'dinner', 'with', 'the', 'red', 'headed', 'girl', '']\n"
     ]
    }
   ],
   "source": [
    "words_break = re.split('\\W+', \"Isn't he coming home for dinner with the red-headed girl?\")\n",
    "print(words_break)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have split `Isn't` to `Isn` and `t`. This is not good if you were working with say email or Twitter data, because you would've a lot more of such contractions. As a minor annoyance, we have an extra empty token at the end. \n",
    "\n",
    "Similarly, because we neglected punctuation `red-headed` is broken into two words: `red` and `headed`\n",
    "\n",
    "We can write custom rules in our tokenization strategy to cover all these cases. Or, use something which already has been written for us. \n",
    "\n",
    "### spaCy for Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above syntax creates a spaCy object `doc`. The object pre-computes a lot of linguistic features, including tokens. \n",
    "\n",
    "We can retrieve them by calling the object iterator. Below, we call the iterator and `list` it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[whole, of, her, sex, ., It, was, not, that, he, felt, \n",
      ", any, emotion, akin, to, love, for, Irene, Adler, ., All, emotions, ,, and, that, \n",
      ", one, particularly, ,, were, abhorrent, to, his, cold, ,, precise, but, \n",
      ", admirably, balanced, mind, ., He, was, ,, I, take, it, ,]\n"
     ]
    }
   ],
   "source": [
    "print(list(doc)[150:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, spaCy tokenizes all *punctuations and words* and returned those as individual tokens as well. Let's try the example which we didn't like earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Is, n't, he, coming, home, for, dinner, with, the, red, -, headed, girl, ?]\n"
     ]
    }
   ],
   "source": [
    "words = nlp(\"Isn't he coming home for dinner with the red-headed girl?\")\n",
    "print([token for token in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Observations*:\n",
    "- spaCy got the `Isn't` split as we wanted \n",
    "- `red-headed` was broken into 3 tokens: `red`, `-`, `headed`. Since the punctuation information isn't lost, we can restore the original `red-headed` token if we want to\n",
    "\n",
    "**How does the spaCy tokenizer work ?**\n",
    "\n",
    "> First, the raw text is split on whitespace characters, similar to text.split(' '). Then, the tokenizer processes the text from left to right. On each substring, it performs two checks:\n",
    "> \n",
    "> - **Does the substring match a tokenizer exception rule?** For example, \"don't\" does not contain whitespace, but should be split into two tokens, \"do\" and \"n't\", while \"U.K.\" should always remain one token.\n",
    "> - **Can a prefix, suffix or infix be split off?** For example punctuation like commas, periods, hyphens or quotes.\n",
    ">\n",
    "> ![caption](https://spacy.io/assets/img/tokenization.svg)\n",
    "> from [spaCy-101](https://spacy.io/usage/spacy-101) docs\n",
    "\n",
    "We can also use spaCy to extract one sentence at a time, instead of one-word-at-a-time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I. A SCANDAL IN BOHEMIA\n",
      "\n",
      "I.\n",
      "\n",
      "To Sherlock Holmes, she is always THE woman., I have seldom heard\n",
      "him mention her under any other name., In his eyes she eclipses\n",
      "and predominates the whole of her sex., It was not that he felt\n",
      "any emotion akin to love for Irene Adler.]\n"
     ]
    }
   ],
   "source": [
    "sentences = list(doc.sents)\n",
    "print(sentences[13:18])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
