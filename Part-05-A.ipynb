{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modern Methods for Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Classification is a very frequently seen challenge. This has several applications ranging from sentiment analysis, tagging and filtering news articles, and detecting fraud reviews (on websites such as Amazon) to name a few. \n",
    "\n",
    "For simplicity, we will be working with a sentiment analysis dataset. For same reasons, we evaluate accuracy as our only metric here. \n",
    "\n",
    "We have a curated set of movie reviews picked up from Imdb. Each review is marked as either positive or negative. Ofcourse, this marks the overall sentiment of each review. \n",
    "\n",
    "Beginning this section, we will incorporate more and more Machine Learning instead of relying on linguistics analysis alone. In writing this, I assume that you have some basic familiarity with Python packages like [scikit-learn](http://scikit-learn.org/). \n",
    "\n",
    "If you don't, that's fine too. The intent here is too give you a quick reference of how these APIs functions work and save your time in looking up what to learn. \n",
    "\n",
    "You can and must learn to use such functions well, even without knowing all the nitty gritty of underlying math. You can trust these functions as black boxes.\n",
    "\n",
    "### Simple Classifiers\n",
    "\n",
    "We begin by simply tries a few machine learning classifiers such as Logistic Regression, Naive Bayes, Decision Trees. \n",
    "Next, we try Random Forest and Extra Trees Classifier. For all of these implementations, we don't use anything except scikit-learn. \n",
    "\n",
    "\n",
    "### Optimizing Simple Classifiers\n",
    "\n",
    "We can tweak the simple classifiers above to improve their performance. For this, the most common method is to try several slightly different versions of the classifier. We do this by changing the parameters of our classifier. \n",
    "\n",
    "We will learn how to automate this \"search\" process for the best classifier parameters using *GridSearch* and *RandomizedSearch*\n",
    "\n",
    "### Ensemble Methods\n",
    "\n",
    "Ensemble several different classifiers means we will be using a group of models. It is a very popular and easy to understand machine learning technique. This is part of almost every winning Kaggle competition. \n",
    "\n",
    "Despite initial concerns of why this might be slow, some teams working on commercial software have begun using this in production software as well. This is because it requires very little overhead, is easy to parallelize, and allows for a built-in fallback of using a single model. \n",
    "\n",
    "We will look at some of the simplest ensembling techniques based on simple majority, also known as voting ensemble and build using that.\n",
    "\n",
    "In summary, this **Machine Learning for NLP** section covers simple classifiers, parameter optimization, and ensemble methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import gzip\n",
    "from urllib.request import urlretrieve\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import numpy as np\n",
    "# if you are using the fastAI environment, all of these imports work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TqdmUpTo(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None: self.total = tsize\n",
    "        self.update(b * bsize - self.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, filename):\n",
    "    \"\"\"\n",
    "    Download data if the filename does not exist already\n",
    "    Uses Tqdm to show download progress\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "\n",
    "        dirname = os.path.dirname(filename)\n",
    "        if not os.path.exists(dirname):\n",
    "            os.makedirs(dirname)\n",
    "\n",
    "        with TqdmUpTo(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\n",
    "            urlretrieve(url, filename, reporthook=t.update_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download some data:\n",
    "data_url = 'http://files.fast.ai/data/aclImdb.tgz'\n",
    "get_data(data_url, 'data/imdb.tgz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's extract the files above and see what the directory contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "train\n",
      "all\n",
      "neg\n",
      "pos\n",
      "all\n",
      "neg\n",
      "pos\n",
      "unsup\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(os.getcwd())/'data'/'imdb'/'aclImdb'\n",
    "assert data_path.exists()\n",
    "for pathroute in os.walk(data_path):\n",
    "    next_path = pathroute[1]\n",
    "    for stop in next_path:\n",
    "        print(stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This really badly written utility tells us that there are atleast two folders: `train` and `test`. Each of these folders in turn has atleast 3 folders:\n",
    "```bash\n",
    "Test\n",
    "|- all\n",
    "|- neg\n",
    "|- pos\n",
    "```\n",
    "and\n",
    "\n",
    "```bash\n",
    "Train\n",
    "|- all\n",
    "|- neg\n",
    "|- pos\n",
    "|- unsup\n",
    "```\n",
    "\n",
    "The pos and neg folders contain reviews which are positive and negative respectively. The `unsup` folder stands for unsupervised. They are useful for building language models, specially for Deep Learning. We will not use that here. Similarly, the folder `all` is redundant because these reviews are repeated in pos and neg folders. \n",
    "\n",
    "# Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_path/'train'\n",
    "test_path = data_path/'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dir_path):\n",
    "    \"\"\"read data into pandas dataframe\"\"\"\n",
    "    \n",
    "    def load_dir_reviews(reviews_path):    \n",
    "        files_list = list(reviews_path.iterdir())\n",
    "        reviews = []\n",
    "        for filename in files_list:\n",
    "            f = open(filename, 'r', encoding='utf-8')\n",
    "            reviews.append(f.read())\n",
    "        return pd.DataFrame({'text':reviews})\n",
    "        \n",
    "    \n",
    "    pos_path = dir_path/'pos'\n",
    "    neg_path = dir_path/'neg'\n",
    "    \n",
    "    pos_reviews, neg_reviews = load_dir_reviews(pos_path), load_dir_reviews(neg_path)\n",
    "    \n",
    "    pos_reviews['label'] = 1\n",
    "    neg_reviews['label'] = 0\n",
    "    \n",
    "    merged = pd.concat([pos_reviews, neg_reviews])\n",
    "    merged.reset_index(inplace=True)\n",
    "    \n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_data(train_path)\n",
    "test = read_data(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train['text'], train['label']\n",
    "X_test, y_test = test['text'], test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression\n",
    "The simplest of all, we replicate the exact steps which we saw from Chapter 01. \n",
    "\n",
    "Feature Extraction: \n",
    "- Bag of Words\n",
    "- TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression as LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',LR())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw the Pipeline in our introductory section. Pipeline allows to queue multiple operations in one single Python object.\n",
    "\n",
    "#### !TIP\n",
    "We are able to call functions like `fit`, `predict` and `fit_transform` on our `Pipeline` objects because Pipeline automatically calls the corresponding function of the last component in the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.48 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_clf.fit(X=X_train, y=y_train) # note that .fit function calls are inplace, and the Pipeline is not re-assigned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TK mention fit, fit_transform and partial_fit here \n",
    "- add code examples for partial fit here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_predicted = lr_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned earlier, we are calling the `predict` function on our Pipeline. The test reviews go through under the same pre-processing steps e.g. `CountVectorizer()` and `TfidfTransformer()` here as the reviews during training. \n",
    "\n",
    "This ease of simplicity makes `Pipeline` one of the most frequently used abstractions in software-grade machine learning. Users might prefer to execute each step independently, or build their own Pipeline equivalents in some research/experimentation use cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88316"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_acc = sum(lr_predicted == y_test)/len(lr_predicted)\n",
    "lr_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we find our model accuracy?**\n",
    "Let's take a quick look at what is happening in the line above. \n",
    "\n",
    "Consider that our predictions are: `[1, 1, 1]` and ground truth: `[1, 0, 1]`. The equality would return a simple list of boolean objects like: `[True, False, True]`. When we `sum` a boolean list in Python, it returns the number of True cases - giving us exact count of how many times did our model make correct predictions. \n",
    "\n",
    "Diving this value by the total number of predictions made (or, equally the number of test reviews) gives us our accuracy.\n",
    "\n",
    "Let's write the above two line logic into a simple, light weight function to calculate accuracy. This would prevent us from repeating the logic.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imdb_acc(pipeline_clf):\n",
    "    predictions = pipeline_clf.predict(X_test)\n",
    "    assert len(y_test) == len(predictions)\n",
    "    return sum(predictions == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf',LR())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.879"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_acc(lr_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase the Ngram Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',LR())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86596"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_acc(lr_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Why is the above called Naive? There are more powerful and complex methods involving Bayesian approaches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB as MNB\n",
    "mnb_clf = Pipeline([('vect', CountVectorizer()), ('clf',MNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81356"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "imdb_acc(mnb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add TF-IDF\n",
    "\n",
    "Now, let's try the above model with TF-IDF as another step after the Bag of Words (Unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',MNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82956"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "imdb_acc(mnb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), ('clf',MNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82992"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "imdb_acc(mnb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Ngram Range from 1 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',MNB())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "imdb_acc(mnb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change Fit Prior to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_clf = Pipeline([('vect', CountVectorizer(stop_words='english', ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',MNB(fit_prior=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8572"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_clf.fit(X=X_train, y=y_train)\n",
    "imdb_acc(mnb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we made small modifications to try out few combinations in our Pipeline. \n",
    "\n",
    "We thought of each combination which might improve our performance. Increasing the `ngram_range` did work, while changing prior from uniform to fitting it (by changing `fit_prior` to False) did not help at all. This approach is tedious, and slightly error-prone because it also relies too much on human intuition of underlying data the machine learning model to be correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we don't try Gaussian Naive Bayes?\n",
    "\n",
    "Gaussian Naive Bayes assumes that the underlying features matrix (our TF-IDF) is densely packed. Owing to the nature of text (where every word is a feature), this is not the case. Our TF-IDF matrix is not densely packed. \n",
    "\n",
    "Additionally, our feature matrix is not even close to a Gaussian distribution.  \n",
    "\n",
    "We don't use Gaussian Naive Bayes for text classification, because it would not meet our requirements and assumptions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine\n",
    "Prior work such as that by [T Joachims](https://www.cs.cornell.edu/people/tj/publications/joachims_98a.pdf) with over 9K citations recommend Support Vector Classifiers for text classification. \n",
    "\n",
    "It's difficult to estimate whether it will be equally effective for us or not based on such literature due to difference in dataset, pre-processing steps. Let's give it a shot nevertheless:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...,\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "# svc_clf.fit(X=X_train, y=y_train)\n",
    "# Wall time: 14min 23s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6562\n",
      "Wall time: 13min 4s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# svc_acc = imdb_acc(svc_clf)\n",
    "# print(svc_acc)\n",
    "# 0.6562\n",
    "# Wall time: 13min 4s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SVM works best with linearly separabale data (looks like our text is usually not linearly separable), we still wanted to give it a try for completeness. \n",
    "\n",
    "Here, SVM does not have a great performance, and took a really long time to train (~150x) of most other classifiers. We will not look at SVM for this particular dataset again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Baseed Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DTC\n",
    "dtc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',DTC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7026"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_clf.fit(X=X_train, y=y_train)\n",
    "imdb_acc(dtc_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "rfc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',RFC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72428"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_clf.fit(X=X_train, y=y_train)\n",
    "imdb_acc(rfc_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier as XTC\n",
    "xtc_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf',XTC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.751"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtc_clf.fit(X=X_train, y=y_train)\n",
    "imdb_acc(xtc_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatically Fine Tuning \n",
    "\n",
    "Let's focus on our best performing model: Logistic Regression and see if we can push it's performance a little more. \n",
    "\n",
    "The best performance for our model was **0.88312** accuracy earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are using the phrases parameter-search and hyperparameter search interchangeably here. This is done to stay consistent with the Deep Learning vocabulary.\n",
    "\n",
    "We want to select the best performing configuration of our pipeline. Each configuration might be diffirent is small ways like removing stop words, including bigrams and trigrams or similar. \n",
    "\n",
    "The total number of such configurations can be fairly large running into few thousands. In addition to manually selecting few combinations to try, we can try all of these several thousand combinations *and* evaluate each combination. \n",
    "\n",
    "This is too slow for most small-scale experiments such as ours. In large experiments, the possible space can run into millions and several days of computing again making it cost and time prohibitive. \n",
    "\n",
    "I strongly urge you to read this blog on [Hyperparameter Tuning](https://www.oreilly.com/ideas/evaluating-machine-learning-models/page/5/hyperparameter-tuning) to become familiar with the vocabulary and ideas in the space beyond what is discussed here. \n",
    "\n",
    "### RandomizedSearch\n",
    "\n",
    "An alternative was proposed by [*Bergstra & Bengio, 2012*](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf). They demonstrated that Random Search across a large hyperparameter space is more effective than manual (as we did for Multinomial Naive Bayes) and often as-effective or more effective than Grid Search. \n",
    "\n",
    "**How do we use it here?**\n",
    "We build on top of the results such as that of Bergstra et al. We break down our parameter search into two steps: \n",
    "Step 1: Randomized Search to go through a wide parameter combination space in a limited number of iterations \n",
    "Step 2: Use the results above to run a GridSearch in that slightly narrow space. \n",
    "\n",
    "We can repeat the above steps till we stop seeing improvements in our results, but we won't do that here. We leave that as an exercise to the reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to prepare the param_grid?\n",
    "TK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(clf__C=[50, 75, 85, 100], \n",
    "                  vect__stop_words=['english', None],\n",
    "                  vect__ngram_range = [(1, 1), (1, 3)],\n",
    "                  vect__lowercase = [True, False],\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(lr_clf, param_distributions=param_grid, n_iter=5, scoring='accuracy', n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does cv do? Adding cv above causes use of StratifiedKFold for evaluation of the scoring metric\n",
    "What does n_iter do? \n",
    "What does scoring do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 46s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words='english',\n",
       "        ...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=5, n_jobs=-1,\n",
       "          param_distributions={'clf__C': [50, 75, 85, 100], 'vect__stop_words': ['english', None], 'vect__ngram_range': [(1, 1), (1, 3)], 'vect__lowercase': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "random_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated cross-validation accuracy: 0.87396\n"
     ]
    }
   ],
   "source": [
    "print(f'Calculated cross-validation accuracy: {random_search.best_score_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compare the performance of this classifier on the ones which we have already seen, we need to train it on complete dataset and test it on the same split as earlier. We do this next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_random_clf = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "        stri...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89916"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_acc(best_random_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the classifier performance improves by more than 1% by simply changing very few parameters. This is amazing. \n",
    "\n",
    "Let's see what parameters are here. In order to compare this, you would need to know the default values for all of the parameters. Alternatively, we can simply look at the parameters from the `param_grid` that we wrote and note the selected parameter values. For everything not in the grid, default values are chosen and remain unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=False, max_df=1.0, max_features=None, min_df=1,\n",
       "          ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
       "          strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "          tokenizer=None, vocabulary=None)),\n",
       " ('tfidf',\n",
       "  TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       " ('clf',\n",
       "  LogisticRegression(C=100, class_weight=None, dual=False, fit_intercept=True,\n",
       "            intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "            penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "            verbose=0, warm_start=False))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_random_clf.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that in the best classifier: \n",
    "    - the chosen C value in clf is 100, \n",
    "    - lowercase is set to False\n",
    "    - removing stop words is bad idea, and\n",
    "    - adding bigrams and trigrams helps\n",
    "    \n",
    "Observations like these are very specific to this dataset and classifier pipeline. In my experience, this can and does vary widely.\n",
    "\n",
    "We can also not assume that the values are always the best value when we run `RandomizedSearch` for so few iterations. The rule of thumb is to run it for **60 iterations** atleast, and use a much larger `param_grid` as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used RandomizedSearch to understand the broad layout of parameters we want to try. We add the best values for some of those to our pipeline itself and continue to experiment with values of other parameters. \n",
    "\n",
    "We will now run GridSearch for these selected parameters. Here, on a whim, I am choosing to include bigrams and trigrams while running grid search over the `parameter C` of LogisticRegression. \n",
    "\n",
    "**!TIP**\n",
    "\n",
    "I have not mentioned what the parameter `C` stands for or how it influences the classifier. This is definitely important to understand while doing manual parameter search. I could notice that changing `C` helps simply by trying out different values. \n",
    "\n",
    "But, our intention here is to automate as much as possible. I instead try varying values in `C` to try during our `RandomizedSearch`. We are trading off human learning time (maybe a few hours) with compute time (maybe a few extra minutes). This mindset saves us time and effort both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,3))), ('tfidf', TfidfTransformer()), ('clf',LR())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = dict(clf__C=[85, 100, 125, 150])\n",
    "grid_search = GridSearchCV(lr_clf, param_grid=param_grid, scoring='accuracy', n_jobs=-1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_estimator_.score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Models \n",
    "**Ensembling models** is a very powerful technique to improve your model performance across a variety of Machine Learning tasks. \n",
    "\n",
    "In the section below, I borrow heavily from the [Kaggle Ensembling Guide](https://mlwave.com/kaggle-ensembling-guide/) written by [MLWave](https://mlwave.com/).\n",
    "\n",
    "I explain why ensembling helps reduce error, or improve accuracy. I demonstrate all the popular techniques on our chosen task and dataset. Each demo includes the ensembling code (borrowed from MLWave) and performance gain from those. \n",
    "\n",
    "To ensure that you understand these techniques, I strongly urge you to try them on a few datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble\n",
    "\n",
    "### Simple Majority (aka Hard Voting)\n",
    "The simplest ensembling technique is perhaps to take a simple majority. This works on the intuition that a single model might make a error on a particular prediction, but several different models are unlikely to make identical errors. \n",
    "\n",
    "Let's look at an example. \n",
    "\n",
    "Ground truth: 1**1**011001\n",
    "\n",
    "Let's assume there are 3 models with only one error for this example\n",
    "\n",
    "Model A Prediction: 1**0**011001\n",
    "\n",
    "Model B Prediction: 1**1**011001\n",
    "\n",
    "Model C Prediction: 1**1**011001\n",
    "\n",
    "The majority votes gives us the correct answer in this example - \n",
    "\n",
    "Majority vote: 1**1**10110011\n",
    "\n",
    "---\n",
    "\n",
    "We have predictions from our models above, let's check the first few predictions from each of those: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('rf', xtc_clf), ('mnb', mnb_clf)], voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('rf', xtc_clf), ('mnb', mnb_clf)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_predictions = voting_clf.predict(X_test)\n",
    "sum(voting_predictions == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "voting_clf = VotingClassifier(estimators=[('lr', lr_clf), ('lr2', lr_clf),('rf', xtc_clf), ('mnb2', mnb_clf),('mnb', mnb_clf)], voting='soft')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "voting_predictions = voting_clf.predict(X_test)\n",
    "sum(voting_predictions == y_test)/len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an improvement from our simple ensembling technique but nothing significant. Why would this be?\n",
    "\n",
    "#### Removing Correlated Classifiers \n",
    "\n",
    "To see this, let us take 3 simple models again. The ground truth is all 1’s:\n",
    "`\n",
    "1111111100 = 80% accuracy\n",
    "1111111100 = 80% accuracy\n",
    "1011111100 = 70% accuracy\n",
    "`\n",
    "\n",
    "These models are highly correlated in their predictions. When we take a majority vote we see no improvement:\n",
    "\n",
    "`\n",
    "1111111100 = 80% accuracy\n",
    "`\n",
    "\n",
    "Now we compare to 3 less-performing, but highly uncorrelated models:\n",
    "\n",
    "`\n",
    "1111111100 = 80% accuracy\n",
    "0111011101 = 70% accuracy\n",
    "1000101111 = 60% accuracy\n",
    "`\n",
    "\n",
    "When we ensemble this with a majority vote we get:\n",
    "\n",
    "`\n",
    "1111111101 = 90% accuracy\n",
    "`\n",
    "\n",
    "We get an improvement which is much higher than any of our individual models. Low correlation between model predictions can lead to better performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(mnb_predicted, lr_predicted)[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
