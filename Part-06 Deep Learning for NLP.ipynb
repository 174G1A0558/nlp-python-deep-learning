{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the earlier section, we used classical machine learning techniques to build our text classifiers. In this chapter, we will replace those with two very popular deep learning techniques: Convolutional Neural Networks and Recurrent Neural Networks. \n",
    "\n",
    "We will build the simplest possible architectures. We assume a general familiarity with CNNs and RNNs and donâ€™t introduce the same again. We share some best practices for building these deep networks. \n",
    "\n",
    "Lastly, we use one of the popular architectures: the Bi-LSTM layers to do some linguistic tasks we shared earlier. \n",
    "\n",
    "\n",
    "Main Headings : \n",
    "- HEADING 1: PyTorch: Intro to Neural Networks\n",
    "- HEADING 2: DL based Classifiers: RNN + Bi-LSTM\n",
    "- HEADING 3: DL based Classifiers: CNN\n",
    "- HEADING 4: DL for Linguistic Tasks\n",
    "\n",
    "Skills learned: For each heading, insert what the reader will learn to DO in this chapter?\n",
    "- SKILL 1: Comfortable with programming in PyTorch \n",
    "- SKILL 2: How to tokenize text and how to use word embeddings that we saw earlier\n",
    "- SKILL 3: What recurrent networks are, and how to use them for text classification; How to stack RNN layers and use bidirectional RNNs to build more-powerful sequence-processing models\n",
    "- SKILL 4: Using CNN for Text Classification\n",
    "- SKILL 5: Using Bi-LSTM models for linguistic tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Introduction\n",
    "- The three main parts: the model architecture, the loss function and the training strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gpu = True\n",
    "if use_gpu:\n",
    "    assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class  CharCNN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(args.num_features, 256, kernel_size=7, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=7, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        )            \n",
    "            \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n",
    "            nn.ReLU()    \n",
    "        )\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=3, stride=3)\n",
    "        )\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(8704, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=args.dropout)\n",
    "        )\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=args.dropout)\n",
    "        )\n",
    "\n",
    "        self.fc3 = nn.Linear(1024, 4)\n",
    "        self.log_softmax = nn.LogSoftmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1) # collapse\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.log_softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM Classifiers\n",
    "- What is a Bi-LSTM? \n",
    "- What is a RNN? \n",
    "- Implement LSTM-only classification example\n",
    "- Implement Bi-LSTM classification example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM for Linguistic Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastAI",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
